---
title: "Nonlinear dimension reduction for high dimensional approximation and inverse problems."
collection: publications
category: other
permalink: /publication/2026-01-14-phd-thesis
excerpt: 'High-dimensional approximation, dimension reduction, inverse problems, random sketching, Poincar√© inequality'
date: 2026-01-14
venue: "HAL"
slidesurl: 'https://alexandre-pasco.github.io/files/publications/2026-01-14-phd-thesis/slides.pdf'
paperurl: 'https://hal.science/view/index/docid/5469205'
bibtexurl: 'https://alexandre-pasco.github.io/files/publications/2026-01-14-phd-thesis/citation.bib'
citation: 'Pasco, A. (2026). Nonlinear dimension reduction for high dimensional approximation and inverse problems.'
---


## Abstract
This thesis is concerned with two types of nonlinear dimension reduction.

In a first part we focus on a model reduction framework for parameter dependent partial differential equations.
In particular in Chapter 2 we introduce a dictionary-based approach to approximate a solution of such equation in the context of an inverse problem, where only few linear measurements on the solution are given.
We introduce a new selection criterion based on random sketching for adaptively selecting a space generated by a few elements of the dictionary.
We show near-optimality with high probability and we provide the practical details to ensure computational efficiency for both building and evaluating this new criterion.
In Chapter 3 we consider an operator interpolation method recently introduced for preconditioning linear model reduction based on Galerkin projection.
This is of particular interest for problems admitting good linear approximation but whose associated projection operator is ill-conditioned.
The contribution of this thesis is to propose a practical construction of these preconditioners, illustrated with numerical experiments.

In a second part we consider low dimensional featuring for approximating high dimensional functions.
In particular in Chapter 4 we consider gradient-based construction of nonlinear feature maps, which leverages Poincar\'e inequalities on nonlinear manifolds to derive a computable objective function.
However, optimizing the latter is in general a difficult non-convex problem.
We thus introduce new quadratic surrogates to this objective function.
Leveraging concentration inequalities, we provide sub-optimality results for a class of feature maps, including polynomials, and a wide class of input probability measures.
In Chapter 5 we extend the approach from the previous chapter to dimension reduction for a family of high dimensional functions (collective dimension reduction).
We then investigate structured forms of feature maps, aiming to leverage the aforementioned gradient-based method and surrogates to learn compositional function networks.

For every chapter we provide open-source implementation of the methods presented therein.